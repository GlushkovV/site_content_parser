{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade pip\n",
    "\n",
    "#!pip3 install re\n",
    "#!pip3 install time\n",
    "#!pip3 install urllib3\n",
    "#!pip3 install requests\n",
    "#!pip3 install logging\n",
    "#!pip3 install pandas\n",
    "#!pip3 install tqdm\n",
    "#!pip3 install beautifulsoup4\n",
    "#!pip3 install selenium-stealth\n",
    "#!pip3 install fake-useragent\n",
    "#!pip3 install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import urllib3\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium_stealth import stealth\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "# отключаем предупреждения\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "# настройка логирования для подавления предупреждений\n",
    "logging.getLogger(\"bs4\").setLevel(logging.CRITICAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс парсера сайтов\n",
    "class SiteContentParserDSP:\n",
    "\n",
    "    # инициализатор\n",
    "    def __init__(self):\n",
    "        # список исключений для отбраковки сайтов по причине недоступности сайта или капчи перед доступом к сайту\n",
    "        self.content_exceptions = ['your request has been denied','проверяем, человек ли вы. это может занять несколько секунд.','сайт заблокирован хостинг-провайдером','error 404 (not found)!!1 404. that’s an error.', \\\n",
    "                                   'подтвердите, что вы человек, выполнив указанное действие.', 'если вы человек, нажмите на похожий цвет', 'антиспам:я не робот', '.err_name_not_resolved', 'err_timed_out', 'нажмите, если вы человек перейдите по ссылке', \\\n",
    "                                   'домен не прилинкован ни к одной из директорий на сервере!', 'проверка recaptcha пожалуйста, пройдите проверку', 'success! success! your new web server is ready to use.', \\\n",
    "                                   'he website has been stopped sorry, this site has been stopped by the administrator', 'parse error: syntax error, unexpected', 'this site is currently under construction. please check back soon', \\\n",
    "                                   'сайт отключен, технические работы сайт был удалён, из-за взлома','пожалуйста, включите javaScript, чтобы продолжить.', 'this site is currently under construction. please check back soon. domain: hd-lord.net', \\\n",
    "                                   'the server encountered an internal error or misconfiguration and was unable to complete your request.', 'сайт оффлайн приносим вам свои извинения за доставленные неудобства']\n",
    "\n",
    "    # функция доступа к контенту сайта\n",
    "    def content_soup(self, site_content):\n",
    "        # проверка успешности запроса\n",
    "        if site_content.status_code == 200:\n",
    "            # извлекаем html код с переданой страницы\n",
    "            soup = BeautifulSoup(site_content.content, 'html.parser')\n",
    "        else:\n",
    "            # если переданный сайт не доступен то пердаем пустой контент\n",
    "            soup = BeautifulSoup('', 'html.parser')\n",
    "            # возвращаем объект формата BeautifulSoup\n",
    "        return soup\n",
    "\n",
    "    # функция очистки текста\n",
    "    def cleaner(self, text):\n",
    "        # условие проверки объекта на существование\n",
    "        if text:\n",
    "            # убираем лишний html синтаксис\n",
    "            text_first_clean = text.strip().replace('\\n', ' ').replace('\\r', ' ').replace('\\xa0', ' ')\n",
    "            # убираем лишние пробелы\n",
    "            text_second_clean = re.sub(r'\\s+', ' ', text_first_clean).strip()\n",
    "            # возврат очищенного текста\n",
    "            return text_second_clean\n",
    "        else:\n",
    "            # возвращаем пустой объект\n",
    "            return None\n",
    "\n",
    "    # функция получения контента страниц с использованием BeautifulSoup\n",
    "    def content_site_beautifulsoup(self, content_soup, url):\n",
    "        # обявляем пустой словарь для итератора\n",
    "        site_content = {}\n",
    "        # добавляем номенование сайта в словарь\n",
    "        site_content['site'] = url\n",
    "        # получаем тайтл которым именуется страница\n",
    "        site_content['title'] = self.cleaner(content_soup.head.title.string) if content_soup.head and content_soup.head.title else None\n",
    "        # получаем описание страницы из раздела мета\n",
    "        site_content['description'] = self.cleaner(content_soup.find('meta', attrs={'name': 'description'})['content']) \\\n",
    "            if content_soup.find('meta', attrs={'name': 'description'}) and 'content' in content_soup.find('meta', attrs={'name': 'description'}).attrs else None\n",
    "        # получаем ключевые слова страницы из раздела мета\n",
    "        site_content['keywords'] = self.cleaner(content_soup.find('meta', attrs={'name': 'keywords'})['content']) \\\n",
    "            if content_soup.find('meta', attrs={'name': 'keywords'}) and 'content' in content_soup.find('meta', attrs={'name': 'keywords'}).attrs else None\n",
    "        # получаем тайтл страницы из раздела мета\n",
    "        site_content['title_meta'] = self.cleaner(content_soup.find('meta', attrs={'property': 'og:title'})['content']) \\\n",
    "            if content_soup.find('meta', attrs={'property': 'og:title'}) and 'content' in content_soup.find('meta', attrs={'property': 'og:title'}).attrs else None\n",
    "        # получаем контент только блока main div как правило это тело страницы за исключением хедера футера и боковых меню\n",
    "        #site_content['content_main'] = self.cleaner(content_soup.select_one('body > main').get_text()) if content_soup.select_one('body > main') else None\n",
    "        # получаем текст всего контента страницы\n",
    "        site_content['full_content'] = self.cleaner(content_soup.get_text()) if content_soup.get_text() != '' else None\n",
    "        # сохраняем оригинал кода страницы без предобработок (возможно будет полезен сайнтистам)\n",
    "        site_content['site_html'] = content_soup if content_soup else None\n",
    "        # возвращаем заполненный словарь\n",
    "        return site_content\n",
    "\n",
    "    # функция очистки адресов\n",
    "    def url_transformer(self, url):\n",
    "        # переменная исключаемого слова\n",
    "        trash_url_part = '.turbopages.org'\n",
    "        # возвращаем или очищенный адрес в случае присутствиия мусорной части или исходный адрес\n",
    "        return url.replace(trash_url_part, \"\").replace('-', '.').replace('..', '-') if trash_url_part in url else url\n",
    "\n",
    "    # функция проверки содержания ошибки в контенте когда сайт не вернул код ошибки но выдал страницу ошибки\n",
    "    def site_excluded(self, content):\n",
    "        # используем .get() для безопасного доступа к значению\n",
    "        full_content = content.get('full_content', '').lower() if content.get('full_content', '') else ''\n",
    "        # проверяем, если full_content пуст или содержит одно из исключений\n",
    "        if full_content == '' or any(exception in full_content for exception in self.content_exceptions):\n",
    "            # возвращаем значения правда, сайт сиключаемый\n",
    "            return True\n",
    "        # возвращаем значения лож, сайт несиключаемый\n",
    "        return False\n",
    "\n",
    "    # функция сбора данных по сыллкам с использованием BeautifulSoup\n",
    "    def frame_content_searcher_bs(self, site_list):\n",
    "        # объявляем пустой список для заполнения результата парсинга\n",
    "        df = []\n",
    "        # объявляем пустой список сылок доступ к которым не получилось получить для дальнейшей попытки обработки через селениум\n",
    "        df_for_selen = []\n",
    "        # цикл перебора списка сайтов\n",
    "        for url in tqdm(site_list, desc=\"Парсинг ссылок BeautifulSoup\", unit=\"ссылка\"):\n",
    "            # испоьзуем фейковые юзер аненты для исключения блокеровки\n",
    "            ua = UserAgent(browsers='chrome')\n",
    "            # присваиваем рандомный агент\n",
    "            user_agent = ua.random\n",
    "            # чистим адрес\n",
    "            url = self.url_transformer(url)\n",
    "            # попытка доступа к странице\n",
    "            try:\n",
    "                # пробуем получить доступ к странице используя http\n",
    "                site_content_html = requests.get('http://' + url, verify=False, timeout=10, headers={'User-Agent':user_agent})\n",
    "                # проверяем на ошибки HTTP\n",
    "                site_content_html.raise_for_status()\n",
    "            # отрабатываем исключение\n",
    "            except requests.RequestException:\n",
    "                try:\n",
    "                    # пробуем получить доступ к странице используя https\n",
    "                    site_content_html = requests.get('https://' + url, verify=False, timeout=10, headers={'User-Agent':user_agent})\n",
    "                    # проверяем на ошибки HTTP\n",
    "                    site_content_html.raise_for_status()\n",
    "                except requests.RequestException:\n",
    "                    # добавляем сылку доступ к которой не получилось получить в список для отработки селениумом\n",
    "                    df_for_selen.append(url)\n",
    "                    # прекращаем выполнения кода в данной итерации и переходим к отработки следующещей сылки\n",
    "                    continue   \n",
    "            # получаем содержимое страницы\n",
    "            site_content_text = self.content_soup(site_content_html)\n",
    "            # получаем данные собранные со страницы\n",
    "            content = self.content_site_beautifulsoup(site_content_text , url)\n",
    "            # проверяем найденный контент на содержание не явных ошибок\n",
    "            if self.site_excluded(content):\n",
    "                # добавляем сылку доступ к которой не получилось получить в список для отработки селениумом\n",
    "                df_for_selen.append(url)\n",
    "                # прекращаем выполнения кода в данной итерации и переходим к отработки следующещей сылки\n",
    "                continue\n",
    "            # добавляем результат сбора данных в спиок\n",
    "            df.append(content)\n",
    "        # возвращем кортеж из двух результирующих списков\n",
    "        return df, df_for_selen\n",
    "\n",
    "    # функция инициализации вебдрайвера с заданными параметарми\n",
    "    def webdriver(self):\n",
    "        # инициализируем юзер агент для сокрытия реальных данных браузера для избежания блокировки\n",
    "        ua = UserAgent(browsers='chrome')\n",
    "        # берем рандомный параметр пользовательского агента\n",
    "        user_agent = ua.random\n",
    "        # инициализируем оъект опций драйвера\n",
    "        options = Options()\n",
    "        # режим работы в фоновом режиме\n",
    "        options.add_argument('--headless=new')\n",
    "        # присваивание выбранного пользовательского агента \n",
    "        options.add_argument(f'--user-agent={user_agent}')\n",
    "        # задаем максимальный размер окна чтобы сайт не переходил на работу в режиме мобильной версии в которой элементы не подгружаются тк расположены иначе\n",
    "        options.add_argument('start-maximized')\n",
    "        # присваеваем окну конкретный размер\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        # задаем параметр отработки графики только на процессоре\n",
    "        options.add_argument('--disable-gpu')\n",
    "        # задаем параметр отключение хранения временных файлов\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        # отключение уведомлений в браузере\n",
    "        options.add_argument('--disable-notifications')\n",
    "        # отключаем блокировку всплывающих окон\n",
    "        options.add_argument('--disable-popup-blocking')\n",
    "        # отключаем режим песочницы\n",
    "        options.add_argument('--no-sandbox')\n",
    "        # активация использования браузеру джава скрипта\n",
    "        options.add_argument('--enable-javascript')\n",
    "        # отклчение детекции джава скриптом браузера как браузера под автоматическоим управлением\n",
    "        options.add_argument( '--disable-blink-features=AutomationControlled')\n",
    "        # добавляется экспериментальная опция, исключающая переключатель \"enable-automation\", чтобы предотвратить автоматическое обнаружение, что браузер управляется вебдрайвером\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "        # добавляется экспериментальная опция, отключающая расширение автоматизации в браузере, что также помогает скрыть использование вебдрайвера\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        # инициализируем вебдрайвер с заданными ранее опциями\n",
    "        driver = Chrome(options=options)                               \n",
    "        # настройка селениум стелс режим для анаоноимности браузера\n",
    "        stealth(driver,\n",
    "                user_agent=user_agent,\n",
    "                languages=['ru-RU', 'ru'],\n",
    "                vendor='Google Inc.',\n",
    "                platform=user_agent.split(' ')[1].strip('()'),\n",
    "                webgl_vendor='Intel Inc.',\n",
    "                renderer='Intel Iris OpenGL Engine',\n",
    "                fix_hairline=True,\n",
    "                run_on_insecure_origins=True\n",
    "                )\n",
    "        # замена прототипа браузера для исключения детекции браузера под автоматическим управлением\n",
    "        driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "            'source':\n",
    "            '''\n",
    "            const newProto = navigator.__proto__;\n",
    "            delete newProto.webdriver;\n",
    "            navigator.__proto__ = newProto;\n",
    "            '''\n",
    "            }\n",
    "        )\n",
    "        # удаляем все куки если они были ранее\n",
    "        driver.delete_all_cookies()\n",
    "        # Установка размеров окна\n",
    "        driver.set_window_size(1920, 1080)\n",
    "        # центровка расположения контента\n",
    "        driver.set_window_position(0, 0)\n",
    "        # возвращаем драйвер\n",
    "        return driver\n",
    "\n",
    "    # функция сбора данных по сыллкам с использованием Selenium\n",
    "    def frame_content_searcher_selen(self, site_list):\n",
    "        # объявляем пустой список для заполнения результата парсинга\n",
    "        df_selen = []\n",
    "        # объявляем пустой список сылок доступ к которым не получилось получить для дальнейшей попытки обработки иными способами\n",
    "        df_for_manual_work = []\n",
    "        # цикл перебора списка сайтов\n",
    "        for url in tqdm(site_list, desc=\"Парсинг ссылок Selenium\", unit=\"ссылка\"):\n",
    "            # создаем экземпляр вебдрайвера\n",
    "            driver = self.webdriver()\n",
    "            # попытка доступа к странице\n",
    "            try:\n",
    "                # пробуем получить доступ к странице используя http передаем url страницы в драйвер\n",
    "                driver.get('http://' + url)\n",
    "                # не явное ожидание загрузки страницы \n",
    "                driver.implicitly_wait(5)\n",
    "                # скролим страницу до конца вверх чтобы загрузились все элементы с которыми будем взаимодействовать\n",
    "                driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "                # явное ожидание для загрузки данных\n",
    "                time.sleep(5)\n",
    "            # отрабатываем исключение\n",
    "            except WebDriverException:\n",
    "                try:\n",
    "                    # пробуем получить доступ к странице используя https передаем url страницы в драйвер\n",
    "                    driver.get('https://' + url)\n",
    "                    # не явное ожидание загрузки страницы \n",
    "                    driver.implicitly_wait(5)\n",
    "                    # скролим страницу до конца вверх чтобы загрузились все элементы с которыми будем взаимодействовать\n",
    "                    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "                    # явное ожидание для загрузки данных\n",
    "                    time.sleep(5)\n",
    "                except WebDriverException:\n",
    "                    # добавляем сылку доступ к которой не получилось получить в список для отработки селениумом\n",
    "                    df_for_manual_work.append(url)\n",
    "                    # прекращаем выполнения кода в данной итерации и переходим к отработки следующещей сылки\n",
    "                    continue\n",
    "            # передаем содержимое вебдрайвера после взаимодействия со страницей из селениума в формате html\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')      \n",
    "            # получаем данные собранные со страницы\n",
    "            content = self.content_site_beautifulsoup(soup , url)\n",
    "            # проверяем найденный контент на содержание не явных ошибок\n",
    "            if self.site_excluded(content):\n",
    "                # добавляем сылку доступ к которой не получилось получить в список для отработки селениумом\n",
    "                df_for_manual_work.append(url)\n",
    "                # прекращаем выполнения кода в данной итерации и переходим к отработки следующещей сылки\n",
    "                continue\n",
    "            # добавляем результат сбора данных в спиок\n",
    "            df_selen.append(content)\n",
    "            # закрываем драйвер\n",
    "            driver.quit()\n",
    "        # возвращем кортеж из двух результирующих списков\n",
    "        return df_selen, df_for_manual_work\n",
    "    \n",
    "    # функция объединяющая сбор данных\n",
    "    def start_parsing(self, site_list):\n",
    "        # вызываем парсинг основного списка через BeautifulSoup\n",
    "        df_bs, df_for_selen_bs = self.frame_content_searcher_bs(site_list)\n",
    "        # вызываем парсинг списка который не был обработан BeautifulSoup через Selenium\n",
    "        df_selen, df_for_manual_work = self.frame_content_searcher_selen(df_for_selen_bs)\n",
    "        # объеденяем списки которые собранные обоими парсерами\n",
    "        df_result = df_bs + df_selen\n",
    "        # возвращаем дата фреймы результирующий и тот что не удалось отработать парсерами для ручной отработки в случае капчи или отбраковки сайта как недоступного\n",
    "        return df_result, df_for_manual_work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Правила использования парсера\n",
    "\n",
    "- Парсер позволяет собирать данные возращая кортеж списков, список успешно собранных данных и список сайтов данные которых собрать не удалось. Доступные варианты использования:\n",
    "    - BeautifulSoup: df_result, df_for_manual_work = SiteContentParserDSP().frame_content_searcher_bs(site_df)\n",
    "    - Selenium: df_result, df_for_manual_work = SiteContentParserDSP().frame_content_searcher_selen(site_df)\n",
    "    - BeautifulSoup & Selenium: df_result, df_for_manual_work = SiteContentParserDSP().start_parsing(site_df)\n",
    "- Рекомендуется использовать вариант BeautifulSoup для повышения колличества собранных данных можно использовать комбенированный метод но это существенно увеличиит время сбора данных и потребуется немного усилий по установке вебдрайвера Chrome в используемую систему для корректной работы библиотеки Selenium. Без этого запуск кода будет не возможен.\n",
    "- В инициализаторе класса присутствует список content_exceptions содержащий список фраз исключений по которым отбраковывается контент содержащий целиком данную фразу. При необходимости список можно дополнить новыми исключениями для более точной отбраковки сайтов. Данные в список заносить только в нижнем регистре без использования заглавных букв.\n",
    "- Если по какойто причине количество ссылок слишком велико для сбора то рекомендуется сбор данных батчем например по 1000 сайтов. site_df[:1000]\n",
    "- Так как цель создания парсера сбор данных для последующей классивифации интересующих сайтов то настоятельно рекомендуется не DDoS-ить сайты поторными запросами. При необходимости сбора данных для класссификации новых сайтов, требуется исключать из выборки для нового парсинга сайты класс и тип контента которых уже определены, это позволит уменьшить время сбора нужных данных, уменьшит нагрузку на собственную сеть и интернет канал, сэкономит ресурсы ПК на котором запускается код и в принципе это будет более этичным поведением в интернет пространстве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем ссылки из файла в формате список\n",
    "site_df = pd.read_excel('dsp_adv_cl-20241011111010.xlsx', skiprows=[0, 2], header=0, usecols=[0])['Domain']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Парсинг ссылок BeautifulSoup: 100%|██████████| 10000/10000 [7:06:26<00:00,  2.56s/ссылка]   \n",
      "Парсинг ссылок Selenium: 100%|██████████| 1250/1250 [4:06:57<00:00, 11.85s/ссылка]   \n"
     ]
    }
   ],
   "source": [
    "# Запуск парсера с использованием BeautifulSoup\n",
    "#df_result, df_for_manual_work = SiteContentParserDSP().frame_content_searcher_bs(site_df[:50])\n",
    "\n",
    "# Запуск парсера с использованием Selenium\n",
    "#df_result, df_for_manual_work = SiteContentParserDSP().frame_content_searcher_selen(site_df[:10])\n",
    "\n",
    "# Запуск парсера комбинированным методом с использованием сбора основных страниц BeautifulSoup и сбором более сложных страниц через Selenium\n",
    "df_result, df_for_manual_work = SiteContentParserDSP().start_parsing(site_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем результирующий фрейм данных в файл текущей датой\n",
    "pd.DataFrame(df_result).to_csv(f\"SiteContentParserDSP_result_{datetime.now().strftime('%d%m%Y')}.csv\")\n",
    "# сохраняем фрейм недоступных сайтов или сайтов с капчей в файл текущей датой\n",
    "pd.DataFrame(df_for_manual_work).to_csv(f\"SiteContentParserDSP_for_manual_work_{datetime.now().strftime('%d%m%Y')}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(pd.DataFrame(df_result))\n",
    "#display(pd.DataFrame(df_for_manual_work))\n",
    "#display(pd.DataFrame(df_result)['full_content'][38])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('SiteContentParserDSP_result_18102024.csv')\n",
    "#df = df.drop(columns=['site_html'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>site</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>keywords</th>\n",
       "      <th>title_meta</th>\n",
       "      <th>full_content</th>\n",
       "      <th>site_html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>vetrenyi.ru</td>\n",
       "      <td>Ветреный (2022) смотреть сериал онлайн бесплат...</td>\n",
       "      <td>Cериал Ветреный смотреть онлайн в хорошем каче...</td>\n",
       "      <td>Ветреный, сериал, смотреть онлайн, бесплатно, ...</td>\n",
       "      <td>Ветреный (2022) смотреть сериал онлайн бесплат...</td>\n",
       "      <td>Ветреный (2022) смотреть сериал онлайн бесплат...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"ru-RU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>netpryshi.ru</td>\n",
       "      <td>Бьюти-Журнал NetPryshi.ru — путь к идеальной коже</td>\n",
       "      <td>Прыщи, черные точки, фурункулы. Особенности ух...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Бьюти-Журнал NetPryshi.ru — путь к идеальной к...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html lang=\"ru-RU\"&gt;\\n&lt;head&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>happylove.top</td>\n",
       "      <td>Отдых в картинках</td>\n",
       "      <td>Смотрите фото онлайн - Отдых в картинках. Тема:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Отдых в картинках Меню ВьетнамГрецияЕгипетИтал...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html lang=\"ru\"&gt;\\n&lt;head&gt;\\n&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>35.ru</td>\n",
       "      <td>Новости Вологды и Вологодской области | 35.ру</td>\n",
       "      <td>Последние свежие новости Вологды и Вологодской...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Новости Вологды и Вологодской области на сайте...</td>\n",
       "      <td>Новости Вологды и Вологодской области | 35.ру ...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html lang=\"ru\"&gt;\\n&lt;head&gt;\\n&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>split.2pdf.com</td>\n",
       "      <td>Split pdf online. Free pdf splitter to separat...</td>\n",
       "      <td>Using our online PDF splitter, you can separat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2pdf.com provides users with a range of free o...</td>\n",
       "      <td>Split pdf online. Free pdf splitter to separat...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;meta c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8923</th>\n",
       "      <td>8923</td>\n",
       "      <td>first-otvet.ru</td>\n",
       "      <td>first-otvet.ru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>first-otvet.ru Не удается получить доступ к са...</td>\n",
       "      <td>&lt;html dir=\"ltr\" lang=\"ru\"&gt;&lt;head&gt;\\n&lt;meta charse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8924</th>\n",
       "      <td>8924</td>\n",
       "      <td>atyrauskaya-oblast.jobcareer.ru</td>\n",
       "      <td>atyrauskaya-oblast.jobcareer.ru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>atyrauskaya-oblast.jobcareer.ru Не удается пол...</td>\n",
       "      <td>&lt;html dir=\"ltr\" lang=\"ru\"&gt;&lt;head&gt;\\n&lt;meta charse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8925</th>\n",
       "      <td>8925</td>\n",
       "      <td>daryo-uz.translate.goog</td>\n",
       "      <td>Google Translate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google TranslateTranslateCan't translate this ...</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;link href=\"https://www.gstatic.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8926</th>\n",
       "      <td>8926</td>\n",
       "      <td>edaplus.info</td>\n",
       "      <td>edaplus.info</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>edaplus.info Не удается получить доступ к сайт...</td>\n",
       "      <td>&lt;html dir=\"ltr\" lang=\"ru\"&gt;&lt;head&gt;\\n&lt;meta charse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8927</th>\n",
       "      <td>8927</td>\n",
       "      <td>uzkl-7814-desktop-kluz.kolesa-team.org</td>\n",
       "      <td>uzkl-7814-desktop-kluz.kolesa-team.org</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uzkl-7814-desktop-kluz.kolesa-team.org Нет под...</td>\n",
       "      <td>&lt;html class=\"offline\" dir=\"ltr\" lang=\"ru\"&gt;&lt;hea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8928 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                    site  \\\n",
       "0              0                             vetrenyi.ru   \n",
       "1              1                            netpryshi.ru   \n",
       "2              2                           happylove.top   \n",
       "3              3                                   35.ru   \n",
       "4              4                          split.2pdf.com   \n",
       "...          ...                                     ...   \n",
       "8923        8923                          first-otvet.ru   \n",
       "8924        8924         atyrauskaya-oblast.jobcareer.ru   \n",
       "8925        8925                 daryo-uz.translate.goog   \n",
       "8926        8926                            edaplus.info   \n",
       "8927        8927  uzkl-7814-desktop-kluz.kolesa-team.org   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Ветреный (2022) смотреть сериал онлайн бесплат...   \n",
       "1     Бьюти-Журнал NetPryshi.ru — путь к идеальной коже   \n",
       "2                                     Отдых в картинках   \n",
       "3         Новости Вологды и Вологодской области | 35.ру   \n",
       "4     Split pdf online. Free pdf splitter to separat...   \n",
       "...                                                 ...   \n",
       "8923                                     first-otvet.ru   \n",
       "8924                    atyrauskaya-oblast.jobcareer.ru   \n",
       "8925                                   Google Translate   \n",
       "8926                                       edaplus.info   \n",
       "8927             uzkl-7814-desktop-kluz.kolesa-team.org   \n",
       "\n",
       "                                            description  \\\n",
       "0     Cериал Ветреный смотреть онлайн в хорошем каче...   \n",
       "1     Прыщи, черные точки, фурункулы. Особенности ух...   \n",
       "2       Смотрите фото онлайн - Отдых в картинках. Тема:   \n",
       "3     Последние свежие новости Вологды и Вологодской...   \n",
       "4     Using our online PDF splitter, you can separat...   \n",
       "...                                                 ...   \n",
       "8923                                                NaN   \n",
       "8924                                                NaN   \n",
       "8925                                                NaN   \n",
       "8926                                                NaN   \n",
       "8927                                                NaN   \n",
       "\n",
       "                                               keywords  \\\n",
       "0     Ветреный, сериал, смотреть онлайн, бесплатно, ...   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "8923                                                NaN   \n",
       "8924                                                NaN   \n",
       "8925                                                NaN   \n",
       "8926                                                NaN   \n",
       "8927                                                NaN   \n",
       "\n",
       "                                             title_meta  \\\n",
       "0     Ветреный (2022) смотреть сериал онлайн бесплат...   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3     Новости Вологды и Вологодской области на сайте...   \n",
       "4     2pdf.com provides users with a range of free o...   \n",
       "...                                                 ...   \n",
       "8923                                                NaN   \n",
       "8924                                                NaN   \n",
       "8925                                                NaN   \n",
       "8926                                                NaN   \n",
       "8927                                                NaN   \n",
       "\n",
       "                                           full_content  \\\n",
       "0     Ветреный (2022) смотреть сериал онлайн бесплат...   \n",
       "1     Бьюти-Журнал NetPryshi.ru — путь к идеальной к...   \n",
       "2     Отдых в картинках Меню ВьетнамГрецияЕгипетИтал...   \n",
       "3     Новости Вологды и Вологодской области | 35.ру ...   \n",
       "4     Split pdf online. Free pdf splitter to separat...   \n",
       "...                                                 ...   \n",
       "8923  first-otvet.ru Не удается получить доступ к са...   \n",
       "8924  atyrauskaya-oblast.jobcareer.ru Не удается пол...   \n",
       "8925  Google TranslateTranslateCan't translate this ...   \n",
       "8926  edaplus.info Не удается получить доступ к сайт...   \n",
       "8927  uzkl-7814-desktop-kluz.kolesa-team.org Нет под...   \n",
       "\n",
       "                                              site_html  \n",
       "0     <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"ru-RU...  \n",
       "1     <!DOCTYPE html>\\n\\n<html lang=\"ru-RU\">\\n<head>...  \n",
       "2     <!DOCTYPE html>\\n\\n<html lang=\"ru\">\\n<head>\\n<...  \n",
       "3     <!DOCTYPE html>\\n\\n<html lang=\"ru\">\\n<head>\\n<...  \n",
       "4     <!DOCTYPE html>\\n<html lang=\"en\"><head><meta c...  \n",
       "...                                                 ...  \n",
       "8923  <html dir=\"ltr\" lang=\"ru\"><head>\\n<meta charse...  \n",
       "8924  <html dir=\"ltr\" lang=\"ru\"><head>\\n<meta charse...  \n",
       "8925  <html><head><link href=\"https://www.gstatic.co...  \n",
       "8926  <html dir=\"ltr\" lang=\"ru\"><head>\\n<meta charse...  \n",
       "8927  <html class=\"offline\" dir=\"ltr\" lang=\"ru\"><hea...  \n",
       "\n",
       "[8928 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(f\"SiteContentParserDSP_result_2_{datetime.now().strftime('%d%m%Y')}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_parquet('data.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
